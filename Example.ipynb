{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from Shapley import ShapNN\n",
    "from DShap import DShap\n",
    "from shap_utils import *\n",
    "%matplotlib inline\n",
    "MEM_DIR = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a calssification problem and use the a losigitic regression model for a small data set of size 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem, model = 'classification', 'logistic'\n",
    "hidden_units = [] # Empty list in the case of logistic regression.\n",
    "train_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a synthetic data set with input-output $y = Bernouli(f(x))$ where $f$ is a polynomial of oder 'difficulty' and $x \\in \\mathscr{R}^d$. ('important_dims' determines the number of $d$ dimensions in $x$ that are non-null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, difficulty = 50, 1\n",
    "num_classes = 2\n",
    "tol = 0.03\n",
    "target_accuracy = 0.7\n",
    "important_dims = 5\n",
    "clf = return_model(model, solver='liblinear', hidden_units=tuple(hidden_units))\n",
    "_param = 1.0\n",
    "for _ in range(100):\n",
    "    X_raw = np.random.multivariate_normal(mean=np.zeros(d), cov = np.eye(d), \n",
    "                                          size=train_size + 5000)\n",
    "    _, y_raw, _, _ = label_generator(\n",
    "        problem, X_raw, param = _param,  difficulty = difficulty, important=important_dims)\n",
    "    clf.fit(X_raw[:train_size], y_raw[:train_size])\n",
    "    test_acc = clf.score(X_raw[train_size:], y_raw[train_size:])\n",
    "    if test_acc>target_accuracy:\n",
    "        break\n",
    "    _param *= 1.1\n",
    "print('Performance using the whole training set = {0:.2f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the instance that takes cares of computing all the algorithms for the data set. Here we run it several times one-after-another, but in a real-world scenario they could be run in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = X_raw[:train_size], y_raw[:train_size]\n",
    "X_test, y_test = X_raw[train_size:], y_raw[train_size:]\n",
    "model = 'logistic'\n",
    "problem = 'classification'\n",
    "num_test = 1000\n",
    "directory = './temp'\n",
    "dshap = DShap(X, y, X_test, y_test, num_test, sources=sources, model_family=model, metric='accuracy',\n",
    "              directory=directory, seed=0)\n",
    "dshap.run(100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = X_raw[:100], y_raw[:100]\n",
    "X_test, y_test = X_raw[100:], y_raw[100:]\n",
    "model = 'logistic'\n",
    "problem = 'classification'\n",
    "num_test = 1000\n",
    "directory = './temp'\n",
    "dshap = DShap(X, y, X_test, y_test, num_test, model_family=model, metric='accuracy',\n",
    "              directory=directory, seed=1)\n",
    "dshap.run(100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_raw[:100], y_raw[:100]\n",
    "X_test, y_test = X_raw[100:], y_raw[100:]\n",
    "model = 'logistic'\n",
    "problem = 'classification'\n",
    "num_test = 1000\n",
    "directory = './temp'\n",
    "dshap = DShap(X, y, X_test, y_test, num_test, model_family=model, metric='accuracy',\n",
    "              directory=directory, seed=2)\n",
    "dshap.run(100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge results for the parallel runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshap.merge_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the convergence plots of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_plots(dshap.marginals_tmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_plots(dshap.marginals_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the effect of removing high valuen points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dshap.performance_plots([dshap.vals_tmc, dshap.vals_g, dshap.vals_loo], num_plot_markers=20,\n",
    "                       sources=dshap.sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
